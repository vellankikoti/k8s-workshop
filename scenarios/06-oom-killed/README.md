# Scenario 6: OOMKilled

**Difficulty:** Intermediate
**Estimated Time:** 20 minutes
**Image:** `vellankikoti/k8s-masterclass-memory-hog:v1.0`

## Learning Objectives

- Understand Kubernetes resource limits and requests
- Learn about OOMKilled pod termination
- Identify memory limit issues
- Use `kubectl describe` to see OOMKilled events
- Understand container memory management

## The Challenge

Your team deployed an image processing service that handles batch image processing requests. The pod starts successfully, but when processing more than a few images, the pod gets killed and restarts. The pod status shows `OOMKilled` or the pod keeps restarting.

**Your mission:** Find out why the pod is being killed and fix the resource limits!

## Step 1: Deploy the Broken Application

```bash
cd scenarios/06-oom-killed
kubectl apply -f broken/
```

**Expected output:**
```
deployment.apps/image-processor created
service/image-processor-service created
```

## Step 2: Observe the Failure

Wait about 10-15 seconds, then check the pod status:

```bash
kubectl get pods -l app=image-processor
```

**What you should see:**
```
NAME                              READY   STATUS    RESTARTS   AGE
image-processor-xxxxxxxxxx-xxxxx   1/1     Running   0          30s
```

‚úÖ The pod starts fine! But let's trigger the memory issue:

```bash
# Port-forward to access the service
# Note: kubectl port-forward doesn't support -l flag, so get pod name first
POD_NAME=$(kubectl get pod -l app=image-processor -o jsonpath='{.items[0].metadata.name}')
kubectl port-forward $POD_NAME 8080:5000 &
```

In another terminal, trigger image processing:

```bash
curl -X POST http://localhost:8080/process \
  -H "Content-Type: application/json" \
  -d '{"count": 15}'
```

**What you might see:**
- `curl: (52) Empty reply from server` - This is expected! The pod was killed during processing.
- Or the request might hang/timeout

Now check the pod status:

```bash
kubectl get pods -l app=image-processor
```

**What you should see:**

**Option 1: Pod shows OOMKilled (caught immediately):**
```
NAME                              READY   STATUS      RESTARTS   AGE
image-processor-xxxxxxxxxx-xxxxx   0/1     OOMKilled   1          2m
```

**Option 2: Pod shows Running but has restarted (most common):**
```
NAME                              READY   STATUS    RESTARTS   AGE
image-processor-xxxxxxxxxx-xxxxx   1/1     Running   1          2m
```

**Note:** If you see `Running` with `RESTARTS: 1` or more, the pod was OOMKilled and Kubernetes restarted it. This is normal behavior - Kubernetes automatically restarts pods that crash.

**Option 3: Pod shows CrashLoopBackOff (multiple restarts):**
```
NAME                              READY   STATUS             RESTARTS   AGE
image-processor-xxxxxxxxxx-xxxxx   0/1     CrashLoopBackOff   3          3m
```

‚ùå The pod was killed due to out-of-memory! Even if it shows as `Running` now, check the details to confirm OOMKilled occurred.

## Step 3: Investigate

### View Pod Details
```bash
kubectl describe pod -l app=image-processor
```

**Important:** Even if the pod shows as `Running`, check the **Last State** section to see if it was OOMKilled:

Look for the **Last State** section:

```
Last State:     Terminated
  Reason:       OOMKilled
  Exit Code:    137
  Started:      ...
  Finished:     ...
```

**Exit Code 137** = 128 + 9 (SIGKILL) indicates the process was killed by the system (OOM killer).

Also check the **Events** section:

```
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Warning  OOMKilled  30s                kubelet            Memory limit exceeded
```

### Quick Verification Commands

```bash
# Check if pod was OOMKilled (even if now Running)
kubectl describe pod -l app=image-processor | grep -A 5 "Last State"

# Check for OOMKilled in events
kubectl describe pod -l app=image-processor | grep -i "oom"

# Get the exact reason programmatically
kubectl get pod -l app=image-processor -o jsonpath='{.items[0].status.containerStatuses[0].lastState.terminated.reason}'
# Should output: OOMKilled
```

**Understanding Pod Restart Behavior:**
- When a pod is OOMKilled, Kubernetes automatically restarts it
- The pod may show as `Running` after restart, but `RESTARTS: 1` indicates it was killed
- Always check `Last State` or `Events` to confirm OOMKilled occurred

### Check Resource Limits
```bash
kubectl get deployment image-processor -o yaml | grep -A 10 resources
```

**What you should see:**
```yaml
resources:
  requests:
    memory: "64Mi"
    cpu: "100m"
  limits:
    memory: "128Mi"  # This is too low!
    cpu: "500m"
```

### Check Container Memory Usage (if pod is running)
```bash
kubectl top pod -l app=image-processor
```

**Key Questions:**
- What is the memory limit set to?
- What is the actual memory usage?
- What does the OOMKilled reason mean?
- How much memory does the application need?

## Step 4: Troubleshoot

<details>
<summary>üí° Hint 1 - Understand OOMKilled</summary>

OOMKilled means "Out Of Memory Killed". The container exceeded its memory limit and was terminated by the kernel.

</details>

<details>
<summary>üí° Hint 2 - Check the memory limit</summary>

The deployment sets a memory limit. Is it high enough for the application's workload? Processing 15 images might need more than 128Mi.

</details>

<details>
<summary>üí° Hint 3 - Increase the limit</summary>

You need to increase the memory limit in the deployment to allow the application to process more images without being killed.

</details>

<details>
<summary>‚úÖ Solution</summary>

### The Problem

The deployment has a memory limit of `128Mi`, which is too low for the image processing workload. When processing multiple images (each image processing operation uses ~10-15MB), the container's memory usage exceeds the limit, causing the Linux kernel's OOM (Out Of Memory) killer to terminate the container.

**Key Issue:**
- Memory limit: `128Mi` ‚ùå (too low)
- Application needs: ~200-300Mi for processing 15 images
- Result: Container killed when memory limit exceeded

**Exit Code 137:**
- Exit code 137 = 128 + 9 (SIGKILL)
- Indicates the process was killed by the system (OOM killer)

### The Fix

```bash
kubectl apply -f solution/
```

**What changed:**

```yaml
resources:
  requests:
    memory: "128Mi"  # Increased from 64Mi
    cpu: "100m"
  limits:
    memory: "384Mi"  # Increased from 128Mi to 384Mi
    cpu: "500m"
```

### Why This Works

1. **Resource Limits**: Kubernetes enforces memory limits using cgroups
2. **OOM Killer**: When a container exceeds its memory limit, the Linux kernel's OOM killer terminates it
3. **Memory Requests**: Used for scheduling (ensures node has enough memory)
4. **Memory Limits**: Hard limit - container cannot exceed this
5. **Proper Sizing**: Setting appropriate limits prevents OOMKilled while avoiding resource waste

**Key Kubernetes Concept:** Always set resource requests and limits based on actual application needs. Too low = OOMKilled, too high = resource waste. Monitor and adjust based on real usage.

</details>

## Step 5: Verify the Fix

Check that the pod can now handle the workload:

```bash
kubectl get pods -l app=image-processor
```

**Expected output:**
```
NAME                              READY   STATUS    RESTARTS   AGE
image-processor-xxxxxxxxxx-xxxxx   1/1     Running   0          30s
```

Test processing more images:

```bash
# Port-forward if not already running
# Get pod name first (kubectl port-forward doesn't support -l flag)
POD_NAME=$(kubectl get pod -l app=image-processor -o jsonpath='{.items[0].metadata.name}')
kubectl port-forward $POD_NAME 8080:5000 &

# Process 15 images
curl -X POST http://localhost:8080/process \
  -H "Content-Type: application/json" \
  -d '{"count": 15}'
```

**Expected output:**
```json
{"status": "success", "processed": 15, "message": "Successfully processed 15 images"}
```

Check the pod status - it should remain running without restarting:

```bash
kubectl get pods -l app=image-processor
```

**Expected output:**
```
NAME                              READY   STATUS    RESTARTS   AGE
image-processor-xxxxxxxxxx-xxxxx   1/1     Running   0          2m
```

‚úÖ No more OOMKilled!

Monitor memory usage:

```bash
kubectl top pod -l app=image-processor
```

You should see memory usage below the limit.

## Step 6: Cleanup

```bash
kubectl delete -f solution/
```

## What You Learned

‚úÖ Understanding OOMKilled pod termination
‚úÖ How Kubernetes enforces memory limits using cgroups
‚úÖ Using `kubectl describe` to see OOMKilled events
‚úÖ Understanding resource requests vs limits
‚úÖ Setting appropriate memory limits for applications
‚úÖ Exit code 137 meaning (128 + SIGKILL)
‚úÖ Best practice: Set limits based on actual workload requirements

## Common Pitfalls

- **Too low limits**: Causes OOMKilled under normal load
- **Too high limits**: Wastes resources and can prevent scheduling
- **No limits**: Container can consume all node memory
- **Only setting limits**: Should also set requests for proper scheduling
- **Not monitoring**: Don't know actual usage to set proper limits

## Going Deeper

**Kubernetes Documentation:**
- [Resource Management for Pods and Containers](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/)
- [Assign Memory Resources to Containers and Pods](https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/)
- [Configure Quality of Service for Pods](https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/)

**Related Concepts:**
- **Requests**: Minimum resources guaranteed (used for scheduling)
- **Limits**: Maximum resources allowed (enforced by kernel)
- **QoS Classes**: Guaranteed, Burstable, BestEffort (based on requests/limits)
- **cgroups**: Linux kernel feature that enforces limits
- **OOM Score**: How likely a process is to be killed when memory is low

**Try This:**
```bash
# Check current resource usage
kubectl top pod -l app=image-processor

# View resource requests and limits
kubectl describe pod -l app=image-processor | grep -A 5 "Limits\|Requests"

# Check QoS class
kubectl get pod -l app=image-processor -o jsonpath='{.items[0].status.qosClass}'

# Set CPU limits and see how it affects performance
# (CPU throttling vs OOMKilled for memory)
```

**Monitoring Memory:**
```bash
# Watch pod memory usage
watch kubectl top pod -l app=image-processor

# Get detailed resource metrics
kubectl get --raw /apis/metrics.k8s.io/v1beta1/namespaces/default/pods/<pod-name>
```

## Next Challenge

Ready for more? Try **[Scenario 7: Probe Failure](../07-probe-failure/)** to learn about health checks and probes!

---

**Questions or Issues?** Open an issue on [GitHub](https://github.com/vellankikoti/k8s-masterclass/issues)

